{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "from bs4 import BeautifulSoup\n",
    "from progressbar import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修正了只能下載一頁的問題，但偶爾還是會被檔爬蟲，所以謹慎使用\n",
    "# 使用間隔別太短，載第二本請等待至少3分鐘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直接執行以下code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class browser:\n",
    "    def download():       \n",
    "        url = input(\"please enter URL:\")\n",
    "\n",
    "        headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) \\\n",
    "            AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "                Chrome/35.0.1916.114 Safari/537.36',\n",
    "            'Cookie': 'AspxAutoDetectCookieSupport=1'}\n",
    "\n",
    "        response = requests.get(url,headers = headers) #使用header避免訪問受到限制\n",
    "        req = urllib.request.Request(url=url, headers=headers)\n",
    "        html = urllib.request.urlopen(req).read().decode('utf8')\n",
    "        soup = BeautifulSoup(html, 'html.parser') # 使用BeautifulSoup整理html原始碼\n",
    "\n",
    "\n",
    "        # #觀看共有幾頁，確定需要幾個迴圈\n",
    "        # len(soup.find_all('div',{'class':'gdtm'}))\n",
    "        # total = len(soup.find_all('div',{'class':'gdtm'}))  #存入total等等計數用\n",
    "\n",
    "\n",
    "        html_page = soup.find_all('table',{'class':'ptt'})  #用來解析共有幾頁\n",
    "        how_page = []  #存入幾頁用\n",
    "        for i in html_page:\n",
    "            how_page += (i.text)\n",
    "\n",
    "        how_page2 = ( len (how_page)-2 )   \n",
    "        #去掉頭尾< > 符號，真實頁數，也就是需要抓幾頁\n",
    "\n",
    "\n",
    "\n",
    "        #這邊自動命名資料夾名稱，如果沒有漢化的就用原本的英文命名\n",
    "        folder_name =[]\n",
    "        if bool( soup.find_all('h1')[1].text) == True:\n",
    "            folder_name = soup.find_all('h1')[1].text\n",
    "            # strip = 移除頭尾空白\n",
    "        else:\n",
    "            folder_name = soup.find_all('h1')[0].text\n",
    "\n",
    "\n",
    "        # 設定資料夾位置\n",
    "        folder_path='C:/' +folder_name\n",
    "\n",
    "        if os.path.exists(folder_path) == False:  # 判斷資料夾是否存在\n",
    "            os.makedirs(folder_path)  # 不存在則創一個       \n",
    "\n",
    "        # 使用opener \n",
    "\n",
    "        opener=urllib.request.build_opener()\n",
    "\n",
    "        header_list = []\n",
    "\n",
    "        #將headers裡面的物件放入header_list\n",
    "        for key, value in headers.items():\n",
    "            header_list.append((key, value))\n",
    "\n",
    "        opener.addheaders = header_list \n",
    "        urllib.request.install_opener(opener)\n",
    "\n",
    "\n",
    "        link = []  #存連結用\n",
    "        count=1 #顯示進度用\n",
    "        total_page = 0  #存取實際幾頁漫畫用\n",
    "\n",
    "        ###################################計算頁數用####################################\n",
    "        for i in range(how_page2):\n",
    "        #總共有幾頁 實際的url迴圈數    \n",
    "            url2 = url+'?p='+str(i)   \n",
    "            response = requests.get(url2,headers = headers) #使用header避免訪問受到限制\n",
    "            req = urllib.request.Request(url=url2, headers=headers)\n",
    "            html = urllib.request.urlopen(req).read().decode('utf8')\n",
    "            soup = BeautifulSoup(html, 'html.parser') # 使用BeautifulSoup整理html原始碼\n",
    "            for j in range(len(soup.find_all('div',{'class':'gdtm'}))):\n",
    "                total_page+=1\n",
    "        #################################################################################\n",
    "\n",
    "\n",
    "        progress = ProgressBar() #顯示進度條\n",
    "        pbar = ProgressBar().start() #顯示進度條\n",
    "\n",
    "        for i in range(how_page2):\n",
    "        #總共有幾頁 實際的url迴圈數    \n",
    "            url2 = url+'?p='+str(i)   \n",
    "            response = requests.get(url2,headers = headers) #使用header避免訪問受到限制\n",
    "            req = urllib.request.Request(url=url2, headers=headers)\n",
    "            html = urllib.request.urlopen(req).read().decode('utf8')\n",
    "            soup = BeautifulSoup(html, 'html.parser') # 使用BeautifulSoup整理html原始碼\n",
    "\n",
    "\n",
    "            html2=[]\n",
    "            for j in range(len(soup.find_all('div',{'class':'gdtm'}))):\n",
    "                html2 += soup.find_all('div',{'class':'gdtm'})[j].find_all('a')      \n",
    "                #尋找div標籤下 class的 gdtm\n",
    "                #不過gdtm是寫死的 估計ehentai之後改版會將這個改掉，這個爬蟲就失效了\n",
    "\n",
    "            for k in html2:\n",
    "                url3 = k.get('href')  #抓到每一頁的漫畫 存進url3\n",
    "                req = urllib.request.Request(url=url3, headers=headers)\n",
    "                html = urllib.request.urlopen(req).read().decode('utf8')\n",
    "                soup = BeautifulSoup(html, 'html.parser') #更新soup\n",
    "                link.append(soup.find_all('img')[4].get('src'))   #陣列[4]寫死的是因為是圖片原始網址\n",
    "                pbar.update( ( count / total_page )*100 )  #顯示進度條\n",
    "                count+=1\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        pbar.finish()\n",
    "\n",
    "\n",
    "        # 開始把link陣列裡的圖片抓下來\n",
    "        x=1 #顯示進度用\n",
    "\n",
    "        progress = ProgressBar() #顯示進度條\n",
    "        pbar = ProgressBar().start() #顯示進度條\n",
    "\n",
    "        for i in range(len(link)):\n",
    "            local = os.path.join( folder_path + '\\ %s.jpg' % x)\n",
    "            urllib.request.urlretrieve(link[i] ,local) #link是下載的網址 local是儲存圖片的檔案位址\n",
    "            pbar.update( ( x / total_page )*100 )  #顯示進度條\n",
    "            time.sleep(1) #睡眠一下避免被檔\n",
    "            x+=1\n",
    "        pbar.finish()\n",
    "        input(\"press enter to exit...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 執行以下輸入網址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please enter URL:https://e-hentai.org/g/1362311/be0eca4897/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "press enter to exit...\n"
     ]
    }
   ],
   "source": [
    "# 雖然依然會有被擋爬蟲的問題\n",
    "\n",
    "# 共兩條進度條\n",
    "# 1為作業進度，2為下載進度\n",
    "browser.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://e-hentai.org/g/1362311/be0eca4897/\n",
    "\n",
    "# https://e-hentai.org/g/1362261/5762725d12/\n",
    "\n",
    "# https://e-hentai.org/g/1327890/eb5a229cdd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
